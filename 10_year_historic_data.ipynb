{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for year 2010\n",
      "Getting data for year 2011\n",
      "Getting data for year 2012\n",
      "Getting data for year 2012\n",
      "Getting data for year 2013\n",
      "Getting data for year 2014\n",
      "Getting data for year 2015\n",
      "Getting data for year 2016\n",
      "Getting data for year 2017\n",
      "Getting data for year 2018\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11792\\2932095376.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Pivot the data to create a summary table with the average temperature and precipitation for each date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mdf_pivot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"datatype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# Rename the columns to match the desired output format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(self, index, columns, values)\u001b[0m\n\u001b[0;32m   7883\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7885\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7887\u001b[0m     _shared_docs[\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(data, index, columns, values)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[0mindexed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mindexed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns_listlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36munstack\u001b[1;34m(self, level, fill_value)\u001b[0m\n\u001b[0;32m   4155\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4157\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4159\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_1d_only_ea_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_unstack_extension_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         unstacker = _Unstacker(\n\u001b[0m\u001b[0;32m    492\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_expanddim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, index, level, constructor)\u001b[0m\n\u001b[0;32m    138\u001b[0m             )\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_selectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36m_make_selectors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Index contains duplicate entries, cannot reshape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomp_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xlwt \n",
    "\n",
    "def get_weather_data(start_date, end_date):\n",
    "    url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "    params = {\n",
    "        \"datasetid\": \"GHCND\",\n",
    "        \"locationid\": \"CITY:IN000025\",\n",
    "        \"startdate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"enddate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"limit\": 1000,\n",
    "        \"datatypeid\": [\"TAVG\", \"PRCP\"],\n",
    "        \"units\": \"metric\",\n",
    "    }\n",
    "    headers = {\"token\": \"xfxflRXrPIjQjaluOUMtuIJgMAFboAdL\"} # Replace YOUR_TOKEN_HERE with your actual token\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    data = response.json()[\"results\"]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Define the start and end date for each year of data\n",
    "start_date = datetime(2010, 1, 1)\n",
    "end_date = start_date + timedelta(days=365)\n",
    "\n",
    "# Create an empty list to store the data\n",
    "weather_data = []\n",
    "\n",
    "# Loop through each year and call the get_weather_data function\n",
    "for i in range(10):\n",
    "    print(f\"Getting data for year {start_date.year}\")\n",
    "    data = get_weather_data(start_date, end_date)\n",
    "    weather_data.extend(data)\n",
    "    start_date = end_date\n",
    "    end_date += timedelta(days=365)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(weather_data)\n",
    "\n",
    "# Filter the data to include only the TAVG and PRCP columns\n",
    "df = df[df[\"datatype\"].isin([\"TAVG\", \"PRCP\"])]\n",
    "\n",
    "# Pivot the data to create a summary table with the average temperature and precipitation for each date\n",
    "df_pivot = df.pivot(index=\"date\", columns=\"datatype\", values=\"value\")\n",
    "\n",
    "# Rename the columns to match the desired output format\n",
    "df_pivot = df_pivot.rename(columns={\"TAVG\": \"avg_temp\", \"PRCP\": \"precipitation\"})\n",
    "\n",
    "# Export the data to an Excel file\n",
    "df_pivot.to_excel(\"delhi_weather_data.xlsx\", index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xl format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for year 2010\n",
      "Getting data for year 2011\n",
      "Getting data for year 2012\n",
      "Getting data for year 2012\n",
      "Getting data for year 2013\n",
      "Getting data for year 2014\n",
      "Getting data for year 2015\n",
      "Getting data for year 2016\n",
      "Getting data for year 2017\n",
      "Getting data for year 2018\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xlwt \n",
    "\n",
    "def get_weather_data(start_date, end_date):\n",
    "    url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "    params = {\n",
    "        \"datasetid\": \"GHCND\",\n",
    "        \"locationid\": \"CITY:IN000009\",\n",
    "        \"startdate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"enddate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"limit\": 1000,\n",
    "        \"datatypeid\": [\"TAVG\", \"PRCP\"],\n",
    "        \"units\": \"metric\",\n",
    "    }\n",
    "    headers = {\"token\": \"OuRZkSMIcYjqzLzNstzMgRGacNpMeZGZ\"} # Replace YOUR_TOKEN_HERE with your actual token\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    data = response.json()[\"results\"]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Define the start and end date for each year of data\n",
    "start_date = datetime(2010, 1, 1)\n",
    "end_date = start_date + timedelta(days=365)\n",
    "\n",
    "# Create an empty list to store the data\n",
    "weather_data = []\n",
    "\n",
    "# Loop through each year and call the get_weather_data function\n",
    "for i in range(10):\n",
    "    print(f\"Getting data for year {start_date.year}\")\n",
    "    data = get_weather_data(start_date, end_date)\n",
    "    weather_data.extend(data)\n",
    "    start_date = end_date\n",
    "    end_date += timedelta(days=365)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(weather_data)\n",
    "\n",
    "# Filter the data to include only the TAVG and PRCP columns\n",
    "df = df[df[\"datatype\"].isin([\"TAVG\", \"PRCP\"])]\n",
    "\n",
    "# Group the data by date and datatype and calculate the mean of each group\n",
    "df_grouped = df.groupby([\"date\", \"datatype\"]).mean().reset_index()\n",
    "\n",
    "# Pivot the data to create a summary table with the average temperature and precipitation for each date\n",
    "df_pivot = df_grouped.pivot(index=\"date\", columns=\"datatype\", values=\"value\")\n",
    "\n",
    "# Rename the columns to match the desired output format\n",
    "df_pivot = df_pivot.rename(columns={\"TAVG\": \"avg_temp\", \"PRCP\": \"precipitation\"})\n",
    "\n",
    "# Export the data to an Excel file\n",
    "df_pivot.to_excel(\"test_weather_data_10years_2010.xlsx\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlwt\n",
      "  Downloading xlwt-1.3.0-py2.py3-none-any.whl (99 kB)\n",
      "     -------------------------------------- 100.0/100.0 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xlwt\n",
      "Successfully installed xlwt-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlwt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for year 2001\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11792\\4172660392.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Getting data for year {start_date.year}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_weather_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mweather_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mstart_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11792\\4172660392.py\u001b[0m in \u001b[0;36mget_weather_data\u001b[1;34m(start_date, end_date)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"results\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'results'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_weather_data(start_date, end_date):\n",
    "    url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "    params = {\n",
    "        \"datasetid\": \"GHCND\",\n",
    "        \"locationid\": \"CITY:IN000008\",\n",
    "        \"startdate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"enddate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"limit\": 1000,\n",
    "        \"datatypeid\": [\"TAVG\", \"PRCP\"],\n",
    "        \"units\": \"metric\",\n",
    "    }\n",
    "    headers = {\"token\": \"xfxflRXrPIjQjaluOUMtuIJgMAFboAdL\"} # Replace YOUR_TOKEN_HERE with your actual token\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    data = response.json()[\"results\"]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Define the start and end date for each year of data\n",
    "start_date = datetime(2001, 1, 1)\n",
    "end_date = start_date + timedelta(days=365)\n",
    "\n",
    "# Create an empty list to store the data\n",
    "weather_data = []\n",
    "\n",
    "# Loop through each year and call the get_weather_data function\n",
    "for i in range(10):\n",
    "    print(f\"Getting data for year {start_date.year}\")\n",
    "    data = get_weather_data(start_date, end_date)\n",
    "    weather_data.extend(data)\n",
    "    start_date = end_date\n",
    "    end_date += timedelta(days=365)\n",
    "try:\n",
    "        data = response.json()[\"results\"]\n",
    "except KeyError:\n",
    "        print(f\"Error: 'results' key not found in API response for dates {start_date} to {end_date}\")\n",
    "        data = []\n",
    "    \n",
    "return data\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(weather_data)\n",
    "\n",
    "# Filter the data to include only the TAVG and PRCP columns\n",
    "df = df[df[\"datatype\"].isin([\"TAVG\", \"PRCP\"])]\n",
    "\n",
    "# Group the data by date and datatype and calculate the mean of each group\n",
    "df_grouped = df.groupby([\"date\", \"datatype\"]).mean().reset_index()\n",
    "\n",
    "# Pivot the data to create a summary table with the average temperature and precipitation for each date\n",
    "df_pivot = df_grouped.pivot(index=\"date\", columns=\"datatype\", values=\"value\")\n",
    "\n",
    "# Rename the columns to match the desired output format\n",
    "df_pivot = df_pivot.rename(columns={\"TAVG\": \"avg_temp\", \"PRCP\": \"precipitation\"})\n",
    "\n",
    "# Export the data to a CSV file\n",
    "df_pivot.to_csv(\"delhi_weather_data_10years_2010.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# Define the API endpoint URL\n",
    "url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "\n",
    "# Set the API parameters\n",
    "params = {\n",
    "    \"datasetid\": \"GHCND\", # GHCND: Global Historical Climatology Network Daily\n",
    "    \"locationid\": \"CITY:IN000002\", # Delhi's location ID\n",
    "    \"startdate\": \"2012,01,01\", # Start date of data retrieval\n",
    "    \"enddate\": \"2021,01,01\", # End date of data retrieval\n",
    "    \"limit\": \"1000\", # Maximum number of records per API call\n",
    "    \"units\": \"metric\", # Metric unit system (e.g., Celsius)\n",
    "    \"datatypeid\": [\"TMIN\", \"TMAX\", \"PRCP\"] # Data types to retrieve (minimum temperature, maximum temperature, precipitation)\n",
    "}\n",
    "\n",
    "# Set the HTTP headers with the API token\n",
    "headers = {\"token\": \"xfxflRXrPIjQjaluOUMtuIJgMAFboAdL\"}\n",
    "\n",
    "# Initialize the list to store all the retrieved data\n",
    "all_data = []\n",
    "\n",
    "# Loop over each year and fetch the weather data for that year\n",
    "for year in range(2012, 2022):\n",
    "    params[\"startdate\"] = f\"{year}-01-01\"\n",
    "    params[\"enddate\"] = f\"{year}-12-31\"\n",
    "    \n",
    "    # Make the API request\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    # Extract the data from the response JSON\n",
    "    data = response.json()[\"results\"]\n",
    "    \n",
    "    # Append the data to the all_data list\n",
    "    all_data += data\n",
    "\n",
    "# Save the data to a CSV file\n",
    "with open(\"delhi_weather_data.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"date\", \"datatype\", \"value\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for data in all_data:\n",
    "        writer.writerow({\n",
    "            \"date\": data[\"date\"],\n",
    "            \"datatype\": data[\"datatype\"],\n",
    "            \"value\": data[\"value\"]\n",
    "        })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
